# These settings were setup so that the transition from nginx to alertmanager can observed in 15 seconds.
# Prometheus evaluates these two alerts once every 10sec and once an alert detected, 
# prometheus will see if that alert is holding for atleast 5 seconds before firing an alert.

groups:
- name: InstanceStatus
  # Interval at which rules are evaluated.
  interval: 10s # defaults to 1m
  rules:
  # Alert for any instance that is unreachable for >5 seconds.
  - alert: NginxMetricsScrapeFailure
    expr: nginx_up == 0
    # Prometheus checks for this alert for 5sec.
    for: 5s
    labels:
      severity: warning
    # Prometheus templates apply here in the annotation and label fields of the alert.
    annotations:
      description: "{{ $labels.instance }} of job {{ $labels.job }} has not been able to scrape Nginx
      metrics for more than 5 seconds."
      summary: "Cannot scrape Nginx metrics. Possible Nginx Server failure"
  
    # Alert for any instance that is unreachable for >5 seconds. (this value is for the sake of easy test only)
  - alert: InstanceDown
    expr: up == 0
    # Prometheus checks for this alert for 5sec.
    for: 5s
    labels:
      severity: critical
    # Prometheus templates apply here in the annotation and label fields of the alert.
    annotations:
      description: "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 seconds."
      summary: "Instance {{ $labels.instance }} down"
